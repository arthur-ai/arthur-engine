"""make transforms global

Revision ID: 8a2f6f0802a3
Revises: 6273338ffe42
Create Date: 2025-11-26 16:03:01.167131

"""

import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

from alembic import op

# revision identifiers, used by Alembic.
revision = "8a2f6f0802a3"
down_revision = "6273338ffe42"
branch_labels = None
depends_on = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "trace_transforms",
        sa.Column("id", sa.UUID(), nullable=False),
        sa.Column("task_id", sa.String(), nullable=False),
        sa.Column("name", sa.String(), nullable=False),
        sa.Column("description", sa.String(), nullable=True),
        sa.Column("definition", postgresql.JSON(astext_type=sa.Text()), nullable=False),
        sa.Column("created_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("updated_at", sa.TIMESTAMP(), nullable=False),
        sa.ForeignKeyConstraint(["task_id"], ["tasks.id"], ondelete="CASCADE"),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        op.f("ix_trace_transforms_id"),
        "trace_transforms",
        ["id"],
        unique=False,
    )
    op.create_index(
        op.f("ix_trace_transforms_task_id"),
        "trace_transforms",
        ["task_id"],
        unique=False,
    )

    # Migrate existing dataset_transforms data to trace_transforms
    # First, get a connection to execute raw SQL
    connection = op.get_bind()

    # Add transform_id column as nullable first
    op.add_column(
        "dataset_transforms",
        sa.Column("transform_id", sa.UUID(), nullable=True),
    )

    # Migrate data: extract from dataset_transforms, transform JSON, insert into trace_transforms
    # Transform the definition JSON:
    # 1. Rename "columns" array to "variables"
    # 2. Within each array element, rename "column_name" field to "variable_name"
    connection.execute(
        sa.text(
            """
        INSERT INTO trace_transforms (id, task_id, name, description, definition, created_at, updated_at)
        SELECT
            gen_random_uuid() as id,
            d.task_id,
            dt.name,
            dt.description,
            json_build_object(
                'variables',
                (
                    SELECT json_agg(
                        json_build_object(
                            'variable_name', col->>'column_name',
                            'span_name', col->>'span_name',
                            'attribute_path', col->>'attribute_path',
                            'fallback', col->>'fallback'
                        )
                    )
                    FROM json_array_elements(dt.definition->'columns') AS col
                )
            ) as definition,
            dt.created_at,
            dt.updated_at
        FROM dataset_transforms dt
        JOIN datasets d ON dt.dataset_id = d.id
    """,
        ),
    )

    # Update dataset_transforms with the new transform_id
    connection.execute(
        sa.text(
            """
        UPDATE dataset_transforms dt
        SET transform_id = tt.id
        FROM trace_transforms tt
        JOIN datasets d ON tt.task_id = d.task_id
        WHERE dt.dataset_id = d.id
        AND dt.name = tt.name
        AND dt.created_at = tt.created_at
    """,
        ),
    )

    # Make transform_id non-nullable now that it's populated
    op.alter_column("dataset_transforms", "transform_id", nullable=False)
    op.drop_constraint(
        op.f("uq_dataset_transforms_dataset_id_name"),
        "dataset_transforms",
        type_="unique",
    )
    op.create_index(
        "idx_dataset_transforms_transform_id",
        "dataset_transforms",
        ["transform_id"],
        unique=False,
    )
    op.create_unique_constraint(
        "uq_dataset_transforms_dataset_id_transform_id",
        "dataset_transforms",
        ["dataset_id", "transform_id"],
    )
    op.create_foreign_key(
        "fk_dataset_transforms_trace_transforms",
        "dataset_transforms",
        "trace_transforms",
        ["transform_id"],
        ["id"],
        ondelete="CASCADE",
    )
    op.drop_column("dataset_transforms", "name")
    op.drop_column("dataset_transforms", "definition")
    op.drop_column("dataset_transforms", "updated_at")
    op.drop_column("dataset_transforms", "description")
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    connection = op.get_bind()

    # Add back the columns to dataset_transforms as nullable first
    op.add_column(
        "dataset_transforms",
        sa.Column("description", sa.VARCHAR(), autoincrement=False, nullable=True),
    )
    op.add_column(
        "dataset_transforms",
        sa.Column(
            "updated_at",
            postgresql.TIMESTAMP(),
            autoincrement=False,
            nullable=True,
        ),
    )
    op.add_column(
        "dataset_transforms",
        sa.Column(
            "definition",
            postgresql.JSON(astext_type=sa.Text()),
            autoincrement=False,
            nullable=True,
        ),
    )
    op.add_column(
        "dataset_transforms",
        sa.Column("name", sa.VARCHAR(), autoincrement=False, nullable=True),
    )

    # Migrate data back from trace_transforms to dataset_transforms
    # Transform the definition JSON back:
    # 1. Rename "variables" array to "columns"
    # 2. Within each array element, rename "variable_name" field to "column_name"
    # Append id to name to handle duplicate names
    connection.execute(
        sa.text(
            """
        UPDATE dataset_transforms dt
        SET
            name = tt.name || '_' || dt.id::text,
            description = tt.description,
            definition = json_build_object(
                'columns',
                (
                    SELECT json_agg(
                        json_build_object(
                            'column_name', var->>'variable_name',
                            'span_name', var->>'span_name',
                            'attribute_path', var->>'attribute_path',
                            'fallback', var->>'fallback'
                        )
                    )
                    FROM json_array_elements(tt.definition->'variables') AS var
                )
            ),
            updated_at = tt.updated_at
        FROM trace_transforms tt
        WHERE dt.transform_id = tt.id
    """,
        ),
    )

    # Make the columns non-nullable now that they're populated
    op.alter_column("dataset_transforms", "name", nullable=False)
    op.alter_column("dataset_transforms", "definition", nullable=False)
    op.alter_column("dataset_transforms", "updated_at", nullable=False)

    op.drop_constraint(
        "fk_dataset_transforms_trace_transforms",
        "dataset_transforms",
        type_="foreignkey",
    )
    op.drop_constraint(
        "uq_dataset_transforms_dataset_id_transform_id",
        "dataset_transforms",
        type_="unique",
    )
    op.drop_index(
        "idx_dataset_transforms_transform_id",
        table_name="dataset_transforms",
    )
    op.create_unique_constraint(
        op.f("uq_dataset_transforms_dataset_id_name"),
        "dataset_transforms",
        ["dataset_id", "name"],
        postgresql_nulls_not_distinct=False,
    )
    op.drop_column("dataset_transforms", "transform_id")
    op.drop_index(op.f("ix_trace_transforms_task_id"), table_name="trace_transforms")
    op.drop_index(op.f("ix_trace_transforms_id"), table_name="trace_transforms")
    op.drop_table("trace_transforms")
    # ### end Alembic commands ###
